{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_2_logistic_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "0babfbfaf6c894c68fcbe35cd6875873",
          "grade": false,
          "grade_id": "cell-9c30f1e63ac50295",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "O11spAREH5iY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2: Logistic Regression and Gradient Checking"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "69d943c7d8973d9297dd6f9c10a78f3a",
          "grade": false,
          "grade_id": "cell-2762c2a88e3706e1",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "q935eZ0zH5ia",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Execute this code block to install dependencies when running on colab\n",
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    from os.path import exists\n",
        "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "cf5b0a5a5ceff37c9de8dd59d902faab",
          "grade": false,
          "grade_id": "cell-c0a45f6dfbe9a832",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "MI2F6iRrH5id",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the first part of the lab we saw how to make predictions of continously varying values with a linear regression model. Lets now turn our focus to binary classification using a simple classification algorithm known as Logistic regression.\n",
        "\n",
        "In linear regression we tried to predict the value of $y$ for an example $\\mathbf{x}$ using a linear function $y=\\mathbf{x}^\\top\\theta$ (where $\\mathbf{x}$ and $\\theta$ are column-vectors). This will clearly not be a great solution for predicting binary-valued labels ($y\\in\\{0,1\\}$). In logistic regression we use a different hypothesis class to try to predict the probability that a given example belongs to the \"1\" class versus the probability that it belongs to the \"0\" class. Specifically, we will try to learn a function of the form:\n",
        "\n",
        "\\begin{align}\n",
        "P(y=1|\\mathbf{x}) &= \\frac{1}{1 + \\exp(-\\mathbf{x}^\\top\\theta)} \\equiv \\sigma(\\mathbf{x}^\\top\\theta),\\\\\n",
        "P(y=0|\\mathbf{x}) &= 1 - P(y=1|\\mathbf{x}).\n",
        "\\end{align}\n",
        " \n",
        "The function $\\sigma(z) \\equiv \\frac{1}{1 + \\exp(-z)}$ is called the \"sigmoid\" or \"logistic\" function. The sigmoid function squashes any real valued input into the range $[0,1]$ enabling us to interprete the output as a probability. Our goal is to search for a value of $\\theta$ so that the probability $P(y=1|\\mathbf{x})=\\sigma(\\mathbf{x}^\\top\\theta)$ is large when $\\mathbf{x}$ belongs to the \"1\" class and small when $\\mathbf{x}$ belongs to the \"0\" class (so that $P(y=0|\\mathbf{x})$ is large). \n",
        "\n",
        "With Linear Regression, the natural cost function was one that measured the sum of squared residuals (the difference between the predicted value and true value). With logisitic regression we have a probabilisitic model, so it makes sense that we use a function that measures the likelihood of the data given the model (note that we want to maximise this function rather than minimise it). As an aside, note that in the case of linear regression if we assume that the data has errors that are IID (independently and identically distributed) according to a Normal distribution, then it can be shown that the maximising the likelihood is exactly the same as minimising the sum of squared residuals. For logistic regression, the likelihood function for a single data point is:\n",
        "\n",
        "\\begin{align}\n",
        "p(y|\\mathbf{x}; \\theta) &= \\sigma(\\mathbf{x}^\\top\\theta)^y(1-\\sigma(\\mathbf{x}^\\top\\theta)^{(1-y)}.\n",
        "\\end{align}\n",
        "\n",
        "For a complete dataset of points $(y_i, \\mathbf{x}_i)$, then the complete likelihood is:\n",
        "\n",
        "\\begin{align}\n",
        "L(\\theta) &= \\prod_i \\sigma(\\mathbf{x}_i^\\top\\theta)^{y_i}(1-\\sigma(\\mathbf{x}_i^\\top\\theta)^{(1-y_i)}\n",
        "\\end{align}\n",
        "\n",
        "However, it is considerably easier to maximise the log-likelihood function:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{l}(\\theta) &= \\log L(\\theta) \\\\\n",
        "                    & = \\log \\prod_i \\sigma(\\mathbf{x}_i^\\top\\theta)^{y_i}(1-\\sigma(\\mathbf{x}_i^\\top\\theta)^{(1-y_i)} \\\\\n",
        "                    & = \\sum_i y_i \\log(\\sigma(\\mathbf{x}_i^\\top\\theta)) + (1-y_i) \\log(1-\\sigma(\\mathbf{x}_i^\\top\\theta))\n",
        "\\end{align}\n",
        "\n",
        "Clearly, maximising the log-likelihood is equivalent to minimising the negative log-likelihood. The negative of the log-likelihood function having the form $-\\sum_i y_i \\log(p) + (1-y_i) \\log(p)$, where p is a function returning the predicted probability of class \"1\", is often called the __\"Binary Cross Entropy\"__ function, __\"Binary Cross Entropy Loss\"__ or sometimes the __\"log loss\"__.\n",
        "\n",
        "For conciseness and computational efficiency, we can write the negative logistic regression log-likelihood function in matrix form. Assuming the $y_i$ are stored in a column vector $\\mathbf{y}$ and the data vectors $x_i$ in the __rows__ of a matrix $\\mathbf{X}$, then: \n",
        "\n",
        "\\begin{align}\n",
        "\\mathrm{NLL}(\\theta) & = -(\\mathbf{y} \\log(\\sigma(\\mathbf{X}\\theta)) + (1-\\mathbf{y}) \\log(1-\\sigma(\\mathbf{X}\\theta)))\n",
        "\\end{align}\n",
        "\n",
        "The gradients of this function are given by:\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_\\theta \\mathrm{NLL}(\\theta) & = \\mathbf{X}^T(\\sigma(\\mathbf{X}\\theta) - \\mathbf{y})\n",
        "\\end{align}"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1921cfe81a296b412a9e7e9225f256fc",
          "grade": false,
          "grade_id": "cell-2e920801eca3f37a",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "1TDm6A1IH5id",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Use the box below to compute the gradients of the negative log-likelihood function $\\nabla_\\theta \\mathrm{NLL}(\\theta)$. You can use `torch.sigmoid()` to apply the sigmoid function.__"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "2bf657d9115afca5f85c2a9913405227",
          "grade": false,
          "grade_id": "cell-1d05572289571209",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "dgu1L9UCH5ie",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# we wouldn't normally do this, but for this lab we want to work in double precision\n",
        "# as we'll need the numerical accuracy later on for doing checks on our gradients:\n",
        "torch.set_default_dtype(torch.float64) \n",
        "\n",
        "def logistic_regression_loss_grad(theta, X, y):\n",
        "    # YOUR CODE HERE\n",
        "    grad = X.t() @ (torch.sigmoid(X @ theta) - y)\n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "e3c122bc0ea83661f4775974a44646e7",
          "grade": true,
          "grade_id": "cell-56edf61ac19106c5",
          "locked": true,
          "points": 4,
          "schema_version": 1,
          "solution": false
        },
        "id": "CtO4h_laH5ig",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "theta = torch.zeros(1)\n",
        "X = torch.Tensor([[1]])\n",
        "y = torch.Tensor([[0]])\n",
        "assert(logistic_regression_loss_grad(theta, X, y) == 0.5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "2636e164d2cd4afd3fef15a1f9ab8afc",
          "grade": false,
          "grade_id": "cell-25aaf7d157a2e997",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "472xDdx_H5ij",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training a Logistic Regressor with real data\n",
        "\n",
        "We'll now try gradient descent using our gradient function on a real dataset from `scikit-learn` called `digits`. \n",
        "\n",
        "The `digits` dataset contains handwritten characters (much like the `MNIST` dataset that you may have heard of - we'll explore `MNIST` in a future lab). As logistic regression is a binary classifier, we'll just use the first 2 characters (0 and 1) from the `digits` dataset, and make our own training and test splits:"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "6431a75f69c02547330e7775148df963",
          "grade": false,
          "grade_id": "cell-9c6e2f914cfddeb5",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "AQjmY_g5H5ij",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "X, y = tuple(torch.Tensor(z) for z in load_digits(2, True)) #convert to pytorch Tensors\n",
        "X = torch.cat((X, torch.ones((X.shape[0], 1))), 1) # append a column of 1's to the X's\n",
        "y = y.reshape(-1, 1) # reshape y into a column vector\n",
        "\n",
        "# We're also going to break the data into a training set for computing the regression parameters\n",
        "# and a test set to evaluate the predictive ability of those parameters\n",
        "perm = torch.randperm(y.shape[0])\n",
        "X_train = X[perm[0:260], :]\n",
        "y_train = y[perm[0:260]]\n",
        "X_test = X[perm[260:], :]\n",
        "y_test = y[perm[260:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1e6f5d66c52a8d510614a26c83f9b43f",
          "grade": false,
          "grade_id": "cell-220b2d0a770d55b3",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "n-4uAU-WH5ik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we have the data, we can use our loss function to try and estimate the optimal parameters for the two-digit classification problem. We'll use `PyTorch`s `torch.nn.functional.binary_cross_entropy_with_logits` function to print out the Binary Cross Entropy of the training data at each iteration, and of the test data once the optimisation is complete. \n",
        "\n",
        "Note: `logits` refers to unscaled probabilities before the sigmoid is applied, so in the `binary_cross_entropy_with_logits` function we just pass in $\\mathbf{X}\\theta$. `PyTorch` does also have a `torch.nn.binary_cross_entropy` method that takes in probabilities, however, as we'll see when implementing neural networks in a later lab, we'll often choose to work with logits as they provide better numerical stability thanks to the _log-sum-exp_ trick. "
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1446595438554769359790f27164e8fc",
          "grade": false,
          "grade_id": "cell-30175a88ab1d4440",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "69XzfwP9H5il",
        "colab_type": "code",
        "outputId": "eb31dde2-84c8-4e9b-ef4d-ca067e565127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "alpha = 0.001\n",
        "theta_gd = torch.rand((X_train.shape[1], 1))\n",
        "for e in range(0, 10):\n",
        "    gr = logistic_regression_loss_grad(theta_gd, X_train, y_train)\n",
        "    theta_gd -= alpha * gr\n",
        "    print(\"Epoch:\", e, \" BCE of training data:\", torch.nn.functional.binary_cross_entropy_with_logits(X_train @ theta_gd, y_train))\n",
        "\n",
        "print(\"Gradient Descent Theta:\", theta_gd.t())\n",
        "print(\"BCE of test data:\", torch.nn.functional.binary_cross_entropy_with_logits(X_test @ theta_gd, y_test))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0  BCE of training data: tensor(67.2637)\n",
            "Epoch: 1  BCE of training data: tensor(36.5096)\n",
            "Epoch: 2  BCE of training data: tensor(8.6379)\n",
            "Epoch: 3  BCE of training data: tensor(0.0923)\n",
            "Epoch: 4  BCE of training data: tensor(0.0535)\n",
            "Epoch: 5  BCE of training data: tensor(0.0412)\n",
            "Epoch: 6  BCE of training data: tensor(0.0293)\n",
            "Epoch: 7  BCE of training data: tensor(0.0174)\n",
            "Epoch: 8  BCE of training data: tensor(0.0064)\n",
            "Epoch: 9  BCE of training data: tensor(0.0011)\n",
            "Gradient Descent Theta: tensor([[ 0.1763,  0.4893, -0.3808, -1.2060,  0.1139,  1.3791,  0.5483,  0.6098,\n",
            "          0.9898, -0.0200, -1.7266, -0.1781,  0.3406, -0.3313,  0.1982,  0.3293,\n",
            "          0.3196,  0.0723, -1.2763,  2.0163,  3.0213, -1.1902,  0.2385,  0.5221,\n",
            "          0.6341,  0.1210, -1.0160,  2.7081,  3.0416, -0.8634, -0.9377,  0.8384,\n",
            "          0.7036, -0.2388, -0.8430,  2.4864,  3.3370, -0.7569, -1.2859,  0.0846,\n",
            "          0.5137, -0.2320, -1.6395,  1.8637,  2.5864, -0.9471, -1.2779,  0.5503,\n",
            "          0.5894,  0.3404, -1.3050, -0.0946,  0.3218, -1.3937,  0.1198,  0.7758,\n",
            "          0.9250,  0.8772, -0.5064, -1.6668, -0.4568,  0.5861,  1.3225,  1.2202,\n",
            "          0.2065]])\n",
            "BCE of test data: tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "7c162297646c249bcf53e3dc9560df73",
          "grade": false,
          "grade_id": "cell-18607d78a082c3aa",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "aKJw_ibiH5in",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What do you observe from running the above? Write your answer in the following block:"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "ec5b74769f52b33d845d70b4ccb3c468",
          "grade": true,
          "grade_id": "cell-d3044f0cbdfe7969",
          "locked": false,
          "points": 3,
          "schema_version": 1,
          "solution": true
        },
        "id": "AqhefJOUH5io",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can observe the BCE loss function been minimized over the iterations. We then test our model against the test set. Since our performs well on the test set we can state that it generalizes well the patterns in the data without being affected by data noise (overfitting)."
      ]
    },
    {
      "metadata": {
        "id": "ARDjHxuzH5ip",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient Checking\n",
        "\n",
        "How can we be sure that our gradient function was correct? We might have made an error in the analytical derivation or in the implementation of that derivation into code. Even though we observed the optimisation process on real data converge (hopefully!), you might have made a subtle error in the implementation...\n",
        "\n",
        "So far we have worked with relatively simple algorithms where it is straightforward to compute the objective function and its gradient with pen-and-paper, and then implement the necessary computations in PyTorch. For more complex models that we will see later, the gradient computation can be notoriously difficult to debug and get right. Sometimes a subtly buggy implementation will manage to learn something that can look surprisingly reasonable (while performing less well than a correct implementation). Thus, even with a buggy implementation, it may not at all be apparent that anything is amiss. In this section, we describe a method for numerically checking the derivatives computed by your code to make sure that your implementation is correct. Carrying out the derivative checking procedure described here will significantly increase your confidence in the correctness of your code.\n",
        "\n",
        "Suppose we want to minimize $J(\\theta)$ as a function of $\\theta$. For this example, suppose $J:\\mathbb{R}\\mapsto\\mathbb{R}$, so that $\\theta \\in \\mathbb{R}$. If we are using gradient descent (or other gradient-based optimisation function), then we usually have implemented some function $g(\\theta)$ that purportedly computes $\\frac{d}{d\\theta}J(\\theta)$.\n",
        "\n",
        "How can we check if our implementation of $g$ is correct?\n",
        "\n",
        "Recall the mathematical definition of the derivative is:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{d}{d\\theta}J(\\theta) = \\lim_{\\epsilon \\rightarrow 0}\n",
        "\\frac{J(\\theta+ \\epsilon) - J(\\theta-\\epsilon)}{2 \\epsilon}.\n",
        "\\end{align}\n",
        "\n",
        "Thus, at any specific value of $\\theta$, we can numerically approximate the derivative as follows:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{J(\\theta+{\\rm EPSILON}) - J(\\theta-{\\rm EPSILON})}{2 \\times {\\rm EPSILON}}\n",
        "\\end{align}\n",
        " \n",
        "In practice, we set ${\\rm EPSILON}$ to a small constant, say around $10^{−4}$. (There is a large range of values of EPSILON values that should work well, but we don’t set ${\\rm EPSILON}$ to be \"extremely\" small, say $10^{−20}$, as that would lead to numerical roundoff errors.)\n",
        "\n",
        "Thus, given a function $g(\\theta)$ that is supposedly computing $\\frac{d}{d\\theta}J(\\theta)$, we can now numerically verify its correctness by checking that\n",
        "\n",
        "\\begin{align}\n",
        "g(\\theta) \\approx\n",
        "\\frac{J(\\theta+{\\rm EPSILON}) - J(\\theta-{\\rm EPSILON})}{2 \\times {\\rm EPSILON}}.\n",
        "\\end{align}\n",
        " \n",
        "The degree to which these two values should approximate each other will depend on the details of $J$. But assuming ${\\rm EPSILON}=10^{−4}$, you’ll usually find that the left- and right-hand sides of the above will agree to at least 4 significant digits (and often many more).\n",
        "\n",
        "Now, consider the case where $\\theta \\in \\mathbb{R}^n$ is a vector rather than a single real number (so that we have $n$ parameters that we want to learn), and $J: \\mathbb{R}^n \\mapsto \\mathbb{R}$. We now generalize our derivative checking procedure to the case where $\\theta$ may be a vector (as in our linear regression and logistic regression examples).\n",
        "\n",
        "Suppose we have a function $g_i(\\theta)$ that purportedly computes $\\frac{\\partial}{\\partial\\theta_i}J(\\theta)$; we’d like to check if $g_i$ is outputting correct derivative values. Let $\\textstyle \\theta^{(i+)} = \\theta + {\\rm EPSILON} \\times \\vec{e}_i$, where\n",
        "\n",
        "\\begin{align}\n",
        "\\vec{e}_i = \\begin{bmatrix}0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "is the $i$-th basis vector (a vector of the same dimension as $\\theta$, with a \"1\" in the $i$-th position and \"0\"s everywhere else). So, $\\theta^{(i+)}$ is the same as $\\theta$, except its $i$-th element has been incremented by ${\\rm EPSILON}$. Similarly, let $\\theta^{(i−)}=\\theta−{\\rm EPSILON} \\times \\vec{e}_i$ be the corresponding vector with the $i$-th element decreased by ${\\rm EPSILON}$.\n",
        "\n",
        "We can now numerically verify $g_i(\\theta)$'s correctness by checking, for each $i$, that:\n",
        "\n",
        "\\begin{align}\n",
        "g_i(\\theta) \\approx\n",
        "\\frac{J(\\theta^{(i+)}) - J(\\theta^{(i-)})}{2 \\times {\\rm EPSILON}}.\n",
        "\\end{align}\n",
        "\n",
        "### Gradient checker code\n",
        "\n",
        "The following code block contains an implementation of the gradient checking proceedure described above. "
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "e150bfbcf79176283150a9c1e1d567fc",
          "grade": false,
          "grade_id": "cell-e035a05e7b6b4f48",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "AQ2c5Iw7H5ip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "\n",
        "def grad_check(f, x, analytic_grad, num_checks=10, h=1e-5):\n",
        "    sum_error = 0\n",
        "    for i in range(num_checks):\n",
        "        ix = tuple([randrange(m) for m in x.shape]) #randomly sample value to change\n",
        "\n",
        "        oldval = x[ix].item()\n",
        "        x[ix] = oldval + h # increment by h\n",
        "        fxph = f(x) # evaluate f(x + h)\n",
        "        x[ix] = oldval - h # increment by h\n",
        "        fxmh = f(x) # evaluate f(x - h)\n",
        "        x[ix] = oldval # reset\n",
        "\n",
        "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
        "        grad_analytic = analytic_grad[ix]\n",
        "        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic) + 1e-8)\n",
        "        sum_error += rel_error\n",
        "        print('numerical: %f\\tanalytic: %f\\trelative error: %e' % (grad_numerical, grad_analytic, rel_error))\n",
        "    return sum_error / num_checks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "895bbc8a80d3679843b7d6fd156d2dd3",
          "grade": false,
          "grade_id": "cell-465484e88bb39e62",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "gLcYX3MhH5ir",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To use the gradient checker, we provide our analytical gradients, together with a function that computes the actual loss (rather than the gradients of the loss) and the parameters at which the gradient was computed:"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "2861f02065d7ae522e4e37116f582c1e",
          "grade": true,
          "grade_id": "cell-6505f3d2a5b382af",
          "locked": true,
          "points": 3,
          "schema_version": 1,
          "solution": false
        },
        "id": "76Z1Mk-rH5ir",
        "colab_type": "code",
        "outputId": "b23ed9cb-c268-40f8-85fd-96aac25539ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "#we'll use random parameters:\n",
        "theta = torch.rand_like(theta_gd)*0.001\n",
        "# and compute the analytic gradient (w.r.t the test data we loaded in this case)\n",
        "grad = logistic_regression_loss_grad(theta, X_test, y_test)\n",
        "\n",
        "# we need a function that computes the loss for a given theta (and implicitly the data)\n",
        "def func(th):\n",
        "    sigm = torch.sigmoid(X_test @ th)\n",
        "    f = -(y_test.t() @ torch.log(sigm) + (1 - y_test.t()) @ torch.log(1 - sigm));\n",
        "    return f\n",
        "\n",
        "# and run the gradient checker\n",
        "relerr = grad_check(func, theta, grad)\n",
        "print(\"average error:\", relerr)\n",
        "\n",
        "assert relerr < 1e-5\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numerical: 207.043074\tanalytic: 207.043074\trelative error: 2.448879e-11\n",
            "numerical: -237.085487\tanalytic: -237.085487\trelative error: 9.936213e-11\n",
            "numerical: 0.000000\tanalytic: 0.000000\trelative error: 0.000000e+00\n",
            "numerical: -175.531161\tanalytic: -175.531161\trelative error: 9.593611e-11\n",
            "numerical: 0.000000\tanalytic: 0.000000\trelative error: 0.000000e+00\n",
            "numerical: 1.655642\tanalytic: 1.655642\trelative error: 3.681200e-10\n",
            "numerical: 0.000000\tanalytic: 0.000000\trelative error: 0.000000e+00\n",
            "numerical: 161.492191\tanalytic: 161.492191\trelative error: 1.918603e-10\n",
            "numerical: 0.000000\tanalytic: 0.000000\trelative error: 0.000000e+00\n",
            "numerical: 0.000000\tanalytic: 0.000000\trelative error: 0.000000e+00\n",
            "average error: tensor([[7.7977e-11]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "fcd74f63e94ee4ae005b1295e46a1f29",
          "grade": false,
          "grade_id": "cell-d2228b469bb3b0e0",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "h9EZFnPMH5it",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Running the above, you should have a very small average error, and the relative error for each trial should also be a very small value."
      ]
    },
    {
      "metadata": {
        "id": "CojoOuXQH5iu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}